{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (4.38.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (8.1.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (8.22.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: decorator in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: stack-data in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: psutil in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: huggingface-hub in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from accelerate) (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: pyyaml in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: filelock in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: networkx in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: requests in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install protobuf==3.20.*\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    if torch.backends.mps.is_available():\n",
    "        DEVICE = 'mps'\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "else:\n",
    "    DEVICE = 'cuda:0'\n",
    "print(\"Device:\", DEVICE)\n",
    "device = torch.device(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (8.1.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (8.22.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: exceptiongroup in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: decorator in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: stack-data in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialization Cell\n",
    "WORKING_ENV = 'PAPERSPACE' # Can be LABS, COLAB, PAPERSPACE, SAGEMAKER\n",
    "USERNAME = '' # If working on Lab Machines - Your college username\n",
    "assert WORKING_ENV in ['LABS', 'COLAB', 'PAPERSPACE', 'SAGEMAKER']\n",
    "\n",
    "if WORKING_ENV == 'PAPERSPACE': # Using Paperspace\n",
    "    !pip install ipywidgets\n",
    "    content_path = '/notebooks/'\n",
    "    data_path = './data/'\n",
    "    \n",
    "else:\n",
    "  raise NotImplementedError()\n",
    "\n",
    "content_path = Path(content_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running locally\n",
    "import os\n",
    "content_path = os.getcwd()\n",
    "data_path = f'{content_path}/data/'\n",
    "content_path = Path(content_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up data and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save predictions to an output file\n",
    "def labels2file(p, outf_path):\n",
    "\twith open(outf_path,'w') as outf:\n",
    "\t\tfor pi in p:\n",
    "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define the custom dataset class\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, is_multiclass=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.is_multiclass = is_multiclass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        # Use 'orig_label' for multi-class and 'label' for binary\n",
    "        if self.is_multiclass:\n",
    "            dict_item = {'text': item['text'], 'label': item['label'], 'orig_label': item['orig_label']}\n",
    "        else:\n",
    "            dict_item = {'text': item['text'], 'label': item['label']}\n",
    "        return dict_item\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts = [item['text'] for item in batch]\n",
    "        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        encodings['labels'] = labels\n",
    "        if self.is_multiclass:\n",
    "            orig_labels = torch.tensor([item['orig_label'] for item in batch], dtype=torch.long)\n",
    "            encodings['orig_labels'] = orig_labels\n",
    "        return encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "class RoBERTaForPCL(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, dropout_rate=0.1, num_frozen_layers=0, is_multiclass=False):\n",
    "        super().__init__(config)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.is_multiclass = is_multiclass\n",
    "        output_dim = 5 if is_multiclass else 1\n",
    "\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, output_dim)\n",
    "\n",
    "        # Freeze specified bottom layers\n",
    "        if num_frozen_layers > 0:\n",
    "            # Freeze embeddings if num_frozen_layers includes them\n",
    "            if num_frozen_layers >= 1:\n",
    "                for param in self.roberta.embeddings.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # Freeze bottom transformer layers as specified by num_frozen_layers\n",
    "            for layer in self.roberta.encoder.layer[:num_frozen_layers]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds,\n",
    "                               output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n",
    "                               return_dict=return_dict)\n",
    "        pooled_output = self.dropout(outputs[1])\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "def get_groupwise_lr_decay_optimizer(model, learning_rate=1e-5, weight_decay=0.01, lr_decay=0.95, num_groups=3):\n",
    "\n",
    "    n_layers = len(model.roberta.encoder.layer)  # Total number of layers\n",
    "    layers_per_group = max(n_layers // num_groups, 1)  # Ensure at least one layer per group\n",
    "\n",
    "    # Initialize grouped parameters list\n",
    "    grouped_parameters = []\n",
    "\n",
    "    # Embeddings parameters\n",
    "    embedding_decayed_lr = learning_rate * (lr_decay ** num_groups)\n",
    "    grouped_parameters.append({\"params\": model.roberta.embeddings.parameters(), 'lr': embedding_decayed_lr})\n",
    "\n",
    "    # Encoder layers parameters\n",
    "    for group_idx in range(num_groups):\n",
    "        # Calculate decayed learning rate for this group\n",
    "        decayed_lr = learning_rate * (lr_decay ** (num_groups - 1 - group_idx))\n",
    "        \n",
    "        # Calculate the start and end layer index for this group\n",
    "        start_layer = group_idx * layers_per_group\n",
    "        end_layer = (group_idx + 1) * layers_per_group if group_idx < num_groups - 1 else n_layers\n",
    "        \n",
    "        # Aggregate parameters from layers in this group\n",
    "        group_params = []\n",
    "        for layer_idx in range(start_layer, end_layer):\n",
    "            group_params.extend(list(model.roberta.encoder.layer[layer_idx].parameters()))\n",
    "        \n",
    "        grouped_parameters.append({\"params\": group_params, 'lr': decayed_lr})\n",
    "\n",
    "    # Pooler parameters\n",
    "    pooled_decayed_lr = learning_rate \n",
    "    grouped_parameters.append({\"params\": model.roberta.pooler.parameters(), 'lr': pooled_decayed_lr})\n",
    "\n",
    "    # Classifier parameters\n",
    "    classifier_lr = learning_rate \n",
    "    grouped_parameters.append({\"params\": model.classifier.parameters(), 'lr': classifier_lr})\n",
    "    \n",
    "    optimizer = AdamW(grouped_parameters, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer and evaluation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            if model.is_multiclass:\n",
    "                # Convert logits to probabilities and then to multi-class predictions\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                # Map multi-class labels to binary\n",
    "                mapped_preds = preds.clone()\n",
    "                mapped_preds[mapped_preds < 2] = 0  # Map 0,1 to 0\n",
    "                mapped_preds[mapped_preds >= 2] = 1  # Map 2,3,4 to 1\n",
    "                preds = mapped_preds\n",
    "\n",
    "            else:\n",
    "                # Convert logits to probabilities and then to binary predictions for binary classification\n",
    "                probs = torch.sigmoid(outputs).squeeze()\n",
    "                preds = (probs > 0.5).long()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Compute classification report based on actual task type\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Not PCL\", \"PCL\"], output_dict=True, zero_division=0)\n",
    "    model.train()\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "class PCLTrainer(Trainer):\n",
    "    def __init__(self, *args, optimizer_function=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if optimizer_function:\n",
    "            self.optimizer = optimizer_function(self.model)\n",
    "            \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    \n",
    "        if model.is_multiclass:\n",
    "            loss_labels = inputs.pop(\"orig_labels\")\n",
    "            outputs = model(**inputs)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(outputs, loss_labels)\n",
    "        else:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(outputs.view(-1), labels.float().view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "            \n",
    "\n",
    "    def evaluate(self, ignore_keys=None):\n",
    "        eval_results = evaluate(self.model, self.tokenizer, self.eval_dataset)\n",
    "        f1_score = eval_results['PCL']['f1-score']\n",
    "        precision = eval_results['PCL']['precision']\n",
    "        recall = eval_results['PCL']['recall']\n",
    "        accuracy = eval_results['accuracy']\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}, F1 Score: {f1_score}, Precision: {precision}, Recall: {recall}\")\n",
    "\n",
    "        # Log the results with wandb\n",
    "        wandb.log({\"eval_f1\": f1_score, \"precision\": precision, \"recall\": recall, \"accuracy\": accuracy})\n",
    "        return {\"eval_f1\": f1_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "\n",
    "\n",
    "def load_datasets(downsample=False):\n",
    "    dpm = DontPatronizeMe('.', '.')\n",
    "    dpm.load_task1()\n",
    "    trids = pd.read_csv('internal_train_par_ids.csv')\n",
    "    teids = pd.read_csv('internal_dev_par_ids.csv')\n",
    "\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    data=dpm.train_task1_df\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(trids)):\n",
    "        parid = trids.par_id[idx]\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        orig_label = int(data.loc[data.par_id == parid].orig_label.values[0])\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label,\n",
    "            'orig_label':orig_label\n",
    "        })\n",
    "\n",
    "    trdf1 = pd.DataFrame(rows)\n",
    "\n",
    "    if downsample:\n",
    "        # downsample negative instances\n",
    "\n",
    "        pcldf = trdf1[trdf1.label==1]\n",
    "        npos = len(pcldf)\n",
    "\n",
    "        training_set1 = pd.concat([pcldf,trdf1[trdf1.label==0][:npos*2]])\n",
    "        trdf1 = training_set1\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        #print(parid)\n",
    "        # select row from original dataset\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        orig_label = int(data.loc[data.par_id == parid].orig_label.values[0])\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label,\n",
    "            'orig_label':orig_label\n",
    "        })\n",
    "\n",
    "    tedf1 = pd.DataFrame(rows)\n",
    "\n",
    "    return trdf1, tedf1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_df, dev_df = load_datasets(downsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malan-picucci\u001b[0m (\u001b[33malans-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"roberta_finetuning.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "      'name': 'f1',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'num_train_epochs': {\n",
    "            'values': [3, 5, 10]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 1e-5, 5e-4, 1e-4]\n",
    "        },\n",
    "        'per_device_train_batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'frozen_layers': {\n",
    "            'values': [0, 1, 4, 8, 10]\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'values': [0, 0.1, 0.3, 0.5]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'scheduler': {\n",
    "            'values': ['linear', 'cosine']\n",
    "        },\n",
    "        'lr_decay': {\n",
    "            'values': [0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "        },\n",
    "        'num_groups': {\n",
    "            'values': [1, 2, 3, 4, 6, 12]\n",
    "        },\n",
    "        'is_multiclass': {\n",
    "            'values': [False, True]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        torch.manual_seed(6)\n",
    "\n",
    "        # Load the datasets\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        train_set = PCLDataset(tokenizer, train_df, is_multiclass=config.is_multiclass)\n",
    "        dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=config.is_multiclass)\n",
    "        dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=config.scheduler,\n",
    "            overwrite_output_dir=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            report_to=\"wandb\",\n",
    "            run_name=\"roberta-finetuning-test\",\n",
    "            remove_unused_columns=False,\n",
    "            logging_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_f1\",\n",
    "            greater_is_better=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "\n",
    "        model = RoBERTaForPCL.from_pretrained('roberta-base', dropout_rate=config.dropout_rate, num_frozen_layers=config.frozen_layers, is_multiclass=config.is_multiclass).to(device)\n",
    "\n",
    "        def optimizer_function(model):\n",
    "            return get_groupwise_lr_decay_optimizer(\n",
    "                model, \n",
    "                learning_rate=config.learning_rate, \n",
    "                weight_decay=config.weight_decay, \n",
    "                lr_decay=config.lr_decay,\n",
    "                num_groups=config.num_groups\n",
    "    )\n",
    "\n",
    "\n",
    "        print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        # Initialize Trainer\n",
    "        trainer = PCLTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_set,\n",
    "            eval_dataset=dev_set,\n",
    "            data_collator=train_set.collate_fn,\n",
    "            tokenizer=tokenizer,\n",
    "            optimizer_function=optimizer_function\n",
    "        )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate()\n",
    "\n",
    "        # Save the best model manually if it's better than the previous best\n",
    "        if results[\"eval_f1\"] > wandb.run.summary.get('best_f1', 0):\n",
    "            wandb.run.summary['best_f1'] = results[\"eval_f1\"]\n",
    "            model_path = os.path.join('./best_model', wandb.run.name) \n",
    "            model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP_CW_NEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2gefh30q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrozen_layers: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tis_multiclass: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay: 0.85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_groups: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alanpicucci/Desktop/Imperial/NLP/nlp_coursework_aia/wandb/run-20240303_185705-2gefh30q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/2gefh30q' target=\"_blank\">dazzling-sweep-39</a></strong> to <a href='https://wandb.ai/alans-team/NLP_CW_NEW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alans-team/NLP_CW_NEW' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/2gefh30q' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/runs/2gefh30q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoBERTaForPCL were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14767105 trainable parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0953034cda42738dc6e1b7c9b0a970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6514, 'grad_norm': 0.34549185633659363, 'learning_rate': 2.777406404176286e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc067573614543e5b4d3d33469f97163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9049665711556829, F1 Score: 0.0, Precision: 0.0, Recall: 0.0\n",
      "{'loss': 0.5661, 'grad_norm': 1.588516116142273, 'learning_rate': 2.0097501541762864e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4270d5c430eb4cd793d6d04cf3b35c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9106972301814709, F1 Score: 0.42813455657492355, Precision: 0.546875, Recall: 0.35175879396984927\n",
      "{'loss': 0.4082, 'grad_norm': 6.188845634460449, 'learning_rate': 1.0608748458237135e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be33cd1a432747bd9d5cb8618c080563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8576886341929322, F1 Score: 0.4808362369337979, Precision: 0.368, Recall: 0.6934673366834171\n",
      "{'loss': 0.3708, 'grad_norm': 1.6785045862197876, 'learning_rate': 2.9321859582371364e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c51185225c04f45949bd5065ee6ec28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8782234957020058, F1 Score: 0.5124282982791587, Precision: 0.41358024691358025, Recall: 0.6733668341708543\n",
      "{'loss': 0.3595, 'grad_norm': 5.092230796813965, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1a14dc08724c9c9b8c6a5016014546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8605539637058262, F1 Score: 0.4859154929577465, Precision: 0.37398373983739835, Recall: 0.6934673366834171\n",
      "{'train_runtime': 173.061, 'train_samples_per_second': 51.571, 'train_steps_per_second': 0.809, 'train_loss': 0.47116973740713936, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc611c4d1ba647389258683d25ddd41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8782234957020058, F1 Score: 0.5124282982791587, Precision: 0.41358024691358025, Recall: 0.6733668341708543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fb305dc11641aab00f67f5b4a42ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▇█▁▄▁▄</td></tr><tr><td>eval_f1</td><td>▁▇████</td></tr><tr><td>precision</td><td>▁█▆▆▆▆</td></tr><tr><td>recall</td><td>▁▅████</td></tr><tr><td>train/epoch</td><td>▁▃▅▆██</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆████</td></tr><tr><td>train/grad_norm</td><td>▁▂█▃▇</td></tr><tr><td>train/learning_rate</td><td>█▆▄▂▁</td></tr><tr><td>train/loss</td><td>█▆▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.87822</td></tr><tr><td>best_f1</td><td>0.51243</td></tr><tr><td>eval_f1</td><td>0.51243</td></tr><tr><td>precision</td><td>0.41358</td></tr><tr><td>recall</td><td>0.67337</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>140</td></tr><tr><td>train/grad_norm</td><td>5.09223</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3595</td></tr><tr><td>train/total_flos</td><td>580603288924278.0</td></tr><tr><td>train/train_loss</td><td>0.47117</td></tr><tr><td>train/train_runtime</td><td>173.061</td></tr><tr><td>train/train_samples_per_second</td><td>51.571</td></tr><tr><td>train/train_steps_per_second</td><td>0.809</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-sweep-39</strong> at: <a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/2gefh30q' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/runs/2gefh30q</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240303_185705-2gefh30q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dmcfnzg6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrozen_layers: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tis_multiclass: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_groups: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alanpicucci/Desktop/Imperial/NLP/nlp_coursework_aia/wandb/run-20240303_190027-dmcfnzg6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/dmcfnzg6' target=\"_blank\">autumn-sweep-40</a></strong> to <a href='https://wandb.ai/alans-team/NLP_CW_NEW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alans-team/NLP_CW_NEW' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/dmcfnzg6' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/runs/dmcfnzg6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoBERTaForPCL were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 28942849 trainable parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216e17ea135146298b74ef179eae9772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5316, 'grad_norm': 13.026575088500977, 'learning_rate': 3.6180339887498953e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58438279b9ce4637bd5de8e8fc4c5628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-112 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7144221585482331, F1 Score: 0.37184873949579833, Precision: 0.2350597609561753, Recall: 0.8894472361809045\n",
      "{'loss': 0.3615, 'grad_norm': 6.944854736328125, 'learning_rate': 2.618033988749895e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1ee3422cb34d69a595f4f29ebe38e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8815663801337154, F1 Score: 0.5320754716981132, Precision: 0.4259818731117825, Recall: 0.7085427135678392\n",
      "{'loss': 0.2639, 'grad_norm': 2.829892158508301, 'learning_rate': 1.3819660112501054e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee93e7b86ecb4cbe8337eba85338e7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8514804202483286, F1 Score: 0.505564387917329, Precision: 0.3697674418604651, Recall: 0.7989949748743719\n",
      "{'loss': 0.1882, 'grad_norm': 11.194293975830078, 'learning_rate': 3.819660112501053e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f333f16b05340f880b58fb199759d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8806112702960841, F1 Score: 0.5335820895522388, Precision: 0.42433234421364985, Recall: 0.7185929648241206\n",
      "{'loss': 0.1371, 'grad_norm': 7.966089725494385, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74f93ecd3b44f3e9f038c3dd33e32ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8629417382999045, F1 Score: 0.5192629815745393, Precision: 0.38944723618090454, Recall: 0.7788944723618091\n",
      "{'train_runtime': 211.9238, 'train_samples_per_second': 42.114, 'train_steps_per_second': 2.642, 'train_loss': 0.2964510083198547, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7be2fca7d4c4d909836c0592f404111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8806112702960841, F1 Score: 0.5335820895522388, Precision: 0.42433234421364985, Recall: 0.7185929648241206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3628d8a02cc84a03992b890b1b463001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁█▇█▇█</td></tr><tr><td>eval_f1</td><td>▁█▇█▇█</td></tr><tr><td>precision</td><td>▁█▆█▇█</td></tr><tr><td>recall</td><td>█▁▅▁▄▁</td></tr><tr><td>train/epoch</td><td>▁▃▅▆██</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆████</td></tr><tr><td>train/grad_norm</td><td>█▄▁▇▅</td></tr><tr><td>train/learning_rate</td><td>█▆▄▂▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.88061</td></tr><tr><td>best_f1</td><td>0.53358</td></tr><tr><td>eval_f1</td><td>0.53358</td></tr><tr><td>precision</td><td>0.42433</td></tr><tr><td>recall</td><td>0.71859</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>560</td></tr><tr><td>train/grad_norm</td><td>7.96609</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1371</td></tr><tr><td>train/total_flos</td><td>523824741307104.0</td></tr><tr><td>train/train_loss</td><td>0.29645</td></tr><tr><td>train/train_runtime</td><td>211.9238</td></tr><tr><td>train/train_samples_per_second</td><td>42.114</td></tr><tr><td>train/train_steps_per_second</td><td>2.642</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-sweep-40</strong> at: <a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/dmcfnzg6' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/runs/dmcfnzg6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240303_190027-dmcfnzg6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 84bpamhg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrozen_layers: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tis_multiclass: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay: 0.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_groups: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alanpicucci/Desktop/Imperial/NLP/nlp_coursework_aia/wandb/run-20240303_190423-84bpamhg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/84bpamhg' target=\"_blank\">celestial-sweep-41</a></strong> to <a href='https://wandb.ai/alans-team/NLP_CW_NEW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alans-team/NLP_CW_NEW' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/sweeps/yn77y8ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/84bpamhg' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/runs/84bpamhg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoBERTaForPCL were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alanpicucci/Desktop/Imperial/NLP/nlp_venv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14770181 trainable parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bac699b6634a938e99cc87b136c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.439, 'grad_norm': 3.554593086242676, 'learning_rate': 4.052700657469775e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dc51d2d0c244c4b3a13d994682df90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9049665711556829, F1 Score: 0.0, Precision: 0.0, Recall: 0.0\n",
      "{'loss': 1.2804, 'grad_norm': 2.181725025177002, 'learning_rate': 1.3509002191565924e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe291d83bb3440fb295c4e65674e2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9049665711556829, F1 Score: 0.0, Precision: 0.0, Recall: 0.0\n",
      "{'loss': 1.2164, 'grad_norm': 4.125517845153809, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95e8d2c2e654ee59fc6b928aa0d4294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9049665711556829, F1 Score: 0.0, Precision: 0.0, Recall: 0.0\n",
      "{'train_runtime': 102.0871, 'train_samples_per_second': 52.455, 'train_steps_per_second': 0.823, 'train_loss': 1.311917395818801, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf579ac4791f4820a03255d5939c5a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9049665711556829, F1 Score: 0.0, Precision: 0.0, Recall: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abd3e612ad640c3b7f4884d509db184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▁▁</td></tr><tr><td>eval_f1</td><td>▁▁▁▁</td></tr><tr><td>precision</td><td>▁▁▁▁</td></tr><tr><td>recall</td><td>▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▅██</td></tr><tr><td>train/global_step</td><td>▁▁▅▅████</td></tr><tr><td>train/grad_norm</td><td>▆▁█</td></tr><tr><td>train/learning_rate</td><td>█▃▁</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.90497</td></tr><tr><td>eval_f1</td><td>0.0</td></tr><tr><td>precision</td><td>0.0</td></tr><tr><td>recall</td><td>0.0</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>84</td></tr><tr><td>train/grad_norm</td><td>4.12552</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2164</td></tr><tr><td>train/total_flos</td><td>348225041790798.0</td></tr><tr><td>train/train_loss</td><td>1.31192</td></tr><tr><td>train/train_runtime</td><td>102.0871</td></tr><tr><td>train/train_samples_per_second</td><td>52.455</td></tr><tr><td>train/train_steps_per_second</td><td>0.823</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-sweep-41</strong> at: <a href='https://wandb.ai/alans-team/NLP_CW_NEW/runs/84bpamhg' target=\"_blank\">https://wandb.ai/alans-team/NLP_CW_NEW/runs/84bpamhg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240303_190423-84bpamhg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id=\"yn77y8ik\", function=tune_hyperparameters, count=3, project=\"NLP_CW_NEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
