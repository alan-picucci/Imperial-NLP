{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (8.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (5.8.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (6.16.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.4)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.1.5)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mEnabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Collecting easynmt\n",
      "  Downloading EasyNMT-2.0.2.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from easynmt) (4.64.1)\n",
      "Requirement already satisfied: transformers<5,>=4.4 in /usr/local/lib/python3.9/dist-packages (from easynmt) (4.21.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from easynmt) (1.12.1+cu116)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from easynmt) (1.23.4)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from easynmt) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from easynmt) (0.1.97)\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from easynmt) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->easynmt) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers<5,>=4.4->easynmt) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5,>=4.4->easynmt) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers<5,>=4.4->easynmt) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5,>=4.4->easynmt) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers<5,>=4.4->easynmt) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers<5,>=4.4->easynmt) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5,>=4.4->easynmt) (2022.10.31)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext->easynmt) (66.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->easynmt) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->easynmt) (8.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers<5,>=4.4->easynmt) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2.1.1)\n",
      "Building wheels for collected packages: easynmt, fasttext\n",
      "  Building wheel for easynmt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easynmt: filename=EasyNMT-2.0.2-py3-none-any.whl size=19904 sha256=59c4552f64246ddd3399b3c84120d96e76dcf9eba88b24155d11a2f1866e98ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/51/19/60/37550e51634162d0317f08725130f360e64b6e9a83a149090c\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4478333 sha256=c2db7f5dce667b57bd8297073987a802f94b2fee1f7a574d8a3dba24830ab05f\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/84/86/c63cf501c46fb575152daee4b937075a5e9b31765c0e620fd4\n",
      "Successfully built easynmt fasttext\n",
      "Installing collected packages: pybind11, fasttext, easynmt\n",
      "Successfully installed easynmt-2.0.2 fasttext-0.9.2 pybind11-2.11.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacremoses) (2022.10.31)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sacremoses) (4.64.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses) (1.2.0)\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!pip install -U easynmt\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.12.1+cu116)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from accelerate) (0.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2.8)\n",
      "Installing collected packages: safetensors, accelerate\n",
      "Successfully installed accelerate-0.27.2 safetensors-0.4.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install protobuf==3.20.*\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    if torch.backends.mps.is_available():\n",
    "        DEVICE = 'mps'\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "else:\n",
    "    DEVICE = 'cuda:0'\n",
    "print(\"Device:\", DEVICE)\n",
    "device = torch.device(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (8.0.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (6.16.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.4)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.1.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Initialization Cell\n",
    "WORKING_ENV = 'PAPERSPACE' # Can be LABS, COLAB, PAPERSPACE, SAGEMAKER\n",
    "USERNAME = '' # If working on Lab Machines - Your college username\n",
    "assert WORKING_ENV in ['LABS', 'COLAB', 'PAPERSPACE', 'SAGEMAKER']\n",
    "\n",
    "if WORKING_ENV == 'PAPERSPACE': # Using Paperspace\n",
    "    !pip install ipywidgets\n",
    "    content_path = '/notebooks/'\n",
    "    data_path = './data/'\n",
    "    \n",
    "else:\n",
    "  raise NotImplementedError()\n",
    "\n",
    "content_path = Path(content_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running locally\n",
    "import os\n",
    "content_path = os.getcwd()\n",
    "data_path = f'{content_path}/data/'\n",
    "content_path = Path(content_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up data and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save predictions to an output file\n",
    "def labels2file(p, outf_path):\n",
    "\twith open(outf_path,'w') as outf:\n",
    "\t\tfor pi in p:\n",
    "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define the custom dataset class\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        dict_item = {'text': item['text'], 'label': item['label']}\n",
    "        return dict_item\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts = [item['text'] for item in batch]\n",
    "        #labels = torch.tensor([item['label'] for item in batch], dtype=torch.float)\n",
    "        real_labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        encodings['labels'] = real_labels\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "\n",
    "\n",
    "def load_datasets(downsample=False):\n",
    "    dpm = DontPatronizeMe('.', '.')\n",
    "    dpm.load_task1()\n",
    "    trids = pd.read_csv('internal_train_par_ids.csv')\n",
    "    teids = pd.read_csv('internal_dev_par_ids.csv')\n",
    "\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    data=dpm.train_task1_df\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(trids)):\n",
    "        parid = trids.par_id[idx]\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "\n",
    "    trdf1 = pd.DataFrame(rows)\n",
    "\n",
    "    if downsample:\n",
    "        # downsample negative instances\n",
    "\n",
    "        pcldf = trdf1[trdf1.label==1]\n",
    "        npos = len(pcldf)\n",
    "\n",
    "        training_set1 = pd.concat([pcldf,trdf1[trdf1.label==0][:npos*2]])\n",
    "        trdf1 = training_set1\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        #print(parid)\n",
    "        # select row from original dataset\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "\n",
    "    tedf1 = pd.DataFrame(rows)\n",
    "\n",
    "    return trdf1, tedf1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_df, dev_df = load_datasets(downsample=True)\n",
    "train_set = PCLDataset(tokenizer, train_df)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "class RoBERTaForPCL(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, dropout_rate=0.1, num_frozen_layers=0):\n",
    "        super().__init__(config)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Freeze specified bottom layers\n",
    "        if num_frozen_layers > 0:\n",
    "            # Freeze embeddings if num_frozen_layers includes them\n",
    "            if num_frozen_layers >= 1:\n",
    "                for param in self.roberta.embeddings.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # Freeze bottom transformer layers as specified by num_frozen_layers\n",
    "            for layer in self.roberta.encoder.layer[:num_frozen_layers]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds,\n",
    "                               output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n",
    "                               return_dict=return_dict)\n",
    "        pooled_output = self.dropout(outputs[1])\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer and evaluation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate(model, tokenizer, data_loader):\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients for evaluation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            \n",
    "            encodings = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in encodings.items()}\n",
    "            labels = batch['label']\n",
    "            \n",
    "            # Forward pass, get logits from the model\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Convert logits to probabilities and then to binary predictions\n",
    "            probs = torch.sigmoid(outputs).squeeze() \n",
    "            preds = (probs > 0.5).long()  # Convert to binary predictions\n",
    "\n",
    "            # Move preds and labels to CPU, convert to lists for classification_report\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Compute and print the classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Not PCL\", \"PCL\"], output_dict=True, zero_division=0)\n",
    "    model.train()  \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "class PCLTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels from inputs and remove them from the dict\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Compute custom loss: Binary Cross-Entropy with Logits\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        loss = loss_fct(outputs.view(-1), labels.float().view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def evaluate(self, ignore_keys=None):\n",
    "        eval_results = evaluate(self.model, self.tokenizer, self.eval_dataset)\n",
    "        f1_score = eval_results['PCL']['f1-score']\n",
    "        precision = eval_results['PCL']['precision']\n",
    "        recall = eval_results['PCL']['recall']\n",
    "        accuracy = eval_results['accuracy']\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}, F1 Score: {f1_score}, Precision: {precision}, Recall: {recall}\")\n",
    "\n",
    "        # Log the results with wandb\n",
    "        wandb.log({\"eval_f1\": f1_score, \"precision\": precision, \"recall\": recall, \"accuracy\": accuracy})\n",
    "        return {\"eval_f1\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"roberta_finetuning.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "      'name': 'f1',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'num_train_epochs': {\n",
    "            'values': [3, 5, 10]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 1e-5, 5e-4, 1e-4]\n",
    "        },\n",
    "        'per_device_train_batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'frozen_layers': {\n",
    "            'values': [0, 1, 4, 8, 10]\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'values': [0, 0.1, 0.3, 0.5]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'scheduler': {\n",
    "            'values': ['linear', 'cosine']\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        torch.manual_seed(6)\n",
    "\n",
    "        # Load the datasets\n",
    "        cache_dir = os.path.join(content_path, 'cache')\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base', cache_dir=cache_dir)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=config.scheduler,\n",
    "            overwrite_output_dir=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            report_to=\"wandb\",\n",
    "            run_name=\"roberta-finetuning-test\",\n",
    "            remove_unused_columns=False,\n",
    "            logging_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_f1\",\n",
    "            greater_is_better=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "\n",
    "        model = RoBERTaForPCL.from_pretrained('roberta-base', cache_dir=cache_dir , dropout_rate=config.dropout_rate, num_frozen_layers=config.frozen_layers).to(device)\n",
    "\n",
    "        print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        # Initialize Trainer\n",
    "        trainer = PCLTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_set,\n",
    "            eval_dataset=dev_set,\n",
    "            data_collator=train_set.collate_fn,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate()\n",
    "\n",
    "        # Save the best model manually if it's better than the previous best\n",
    "        if results[\"eval_f1\"] > wandb.run.summary.get('best_f1', 0):\n",
    "            wandb.run.summary['best_f1'] = results[\"eval_f1\"]\n",
    "            model_path = os.path.join('./best_model', wandb.run.name) \n",
    "            model.save_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: o0z9do5c\n",
      "Sweep URL: https://wandb.ai/alans-team/NLP_CW/sweeps/o0z9do5c\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP_CW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id=\"o0z9do5c\", function=tune_hyperparameters, count=30, project=\"NLP_CW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stage 1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model with upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    dpm = DontPatronizeMe('.', '.')\n",
    "    dpm.load_task1()\n",
    "    trids = pd.read_csv('internal_train_par_ids.csv')\n",
    "    teids = pd.read_csv('internal_dev_par_ids.csv')\n",
    "\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    data=dpm.train_task1_df\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(trids)):\n",
    "        parid = trids.par_id[idx]\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "\n",
    "    trdf1 = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "    traindf_majority = trdf1[trdf1['label'] == 0]\n",
    "    traindf_minority = trdf1[trdf1['label'] == 1]\n",
    "    traindf_minority_oversampled = resample(traindf_minority,\n",
    "                                   replace=True,\n",
    "                                   n_samples=len(traindf_majority),\n",
    "                                   random_state=42)\n",
    "    traindf_combined = pd.concat([traindf_majority, traindf_minority_oversampled])\n",
    "    traindf_combined = traindf_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    trdf1 = traindf_combined\n",
    "    \n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        #print(parid)\n",
    "        # select row from original dataset\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "\n",
    "    tedf1 = pd.DataFrame(rows)\n",
    "\n",
    "    return trdf1, tedf1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_df, dev_df = load_datasets()\n",
    "train_set = PCLDataset(tokenizer, train_df)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoBERTaForPCL were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 28942849 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malan-picucci\u001b[0m (\u001b[33malans-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alanpicucci/Desktop/Imperial/NLP/nlp_coursework_aia/wandb/run-20240303_191011-04cftgcv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alans-team/huggingface/runs/04cftgcv' target=\"_blank\">roberta-finetuning-test</a></strong> to <a href='https://wandb.ai/alans-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alans-team/huggingface' target=\"_blank\">https://wandb.ai/alans-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alans-team/huggingface/runs/04cftgcv' target=\"_blank\">https://wandb.ai/alans-team/huggingface/runs/04cftgcv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bb47a3d1e4478d96e855ab6da51616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3024, 'grad_norm': 10.743956565856934, 'learning_rate': 4.877641290737884e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86fb20cc16b4929a2bbdb8aa63d17fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9130850047755492, F1 Score: 0.5767441860465117, Precision: 0.5367965367965368, Recall: 0.6231155778894473\n",
      "{'loss': 0.1026, 'grad_norm': 37.85075378417969, 'learning_rate': 4.522542485937369e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a825f64ab82f46e0a8e1afc26944f346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9106972301814709, F1 Score: 0.5701149425287356, Precision: 0.5254237288135594, Recall: 0.6231155778894473\n",
      "{'loss': 0.0478, 'grad_norm': 0.017182234674692154, 'learning_rate': 3.969463130731183e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4c49ac5ad7477c94698c7d586e904d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9164278892072588, F1 Score: 0.5430809399477807, Precision: 0.5652173913043478, Recall: 0.5226130653266332\n",
      "{'loss': 0.0231, 'grad_norm': 2.8956139087677, 'learning_rate': 3.272542485937369e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca636be1d7e4d3a8e8a002e5f41d975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9192932187201528, F1 Score: 0.536986301369863, Precision: 0.5903614457831325, Recall: 0.49246231155778897\n",
      "{'loss': 0.0128, 'grad_norm': 0.009542226791381836, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e49256758141bab5e3e192b7ba1a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9149952244508118, F1 Score: 0.5364583333333334, Precision: 0.5567567567567567, Recall: 0.5175879396984925\n",
      "{'loss': 0.006, 'grad_norm': 0.0067993635311722755, 'learning_rate': 1.7274575140626318e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e593c06c514c18a44b31d37ea2b88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9164278892072588, F1 Score: 0.55470737913486, Precision: 0.5618556701030928, Recall: 0.5477386934673367\n",
      "{'loss': 0.0031, 'grad_norm': 0.0008445466519333422, 'learning_rate': 1.0305368692688174e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89bc6fa0aa2487889bdab49e18ab796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9207258834765998, F1 Score: 0.5229885057471264, Precision: 0.610738255033557, Recall: 0.457286432160804\n",
      "{'loss': 0.0013, 'grad_norm': 0.0013111562002450228, 'learning_rate': 4.7745751406263165e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6dc2655d8642eba6c1db3b816a1ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9192932187201528, F1 Score: 0.5493333333333333, Precision: 0.5852272727272727, Recall: 0.5175879396984925\n",
      "{'loss': 0.0016, 'grad_norm': 0.0015656606992706656, 'learning_rate': 1.2235870926211619e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4157cde8c96646b880e30a62c7e800da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9192932187201528, F1 Score: 0.5517241379310345, Precision: 0.5842696629213483, Recall: 0.5226130653266332\n",
      "{'loss': 0.0017, 'grad_norm': 0.0013653360074386, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb56a3e3d5c4838bf8365b0c1ede4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9202483285577842, F1 Score: 0.5593667546174143, Precision: 0.5888888888888889, Recall: 0.5326633165829145\n",
      "{'train_runtime': 1659.3487, 'train_samples_per_second': 68.533, 'train_steps_per_second': 2.145, 'train_loss': 0.05024582424692893, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6260ac71394e389d3888fe0a0d81c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9130850047755492, F1 Score: 0.5767441860465117, Precision: 0.5367965367965368, Recall: 0.6231155778894473\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "#cache_dir = os.path.join(content_path, 'cache')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    weight_decay=0,\n",
    "    lr_scheduler_type='cosine',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-finetuning-test\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "\n",
    "model = RoBERTaForPCL.from_pretrained('roberta-base', dropout_rate=0, num_frozen_layers=8).to(device)\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "# Initialize Trainer\n",
    "trainer = PCLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=dev_set,\n",
    "    data_collator=train_set.collate_fn,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./best_model/stage_1_model_mixed_language/config.json\n",
      "Model weights saved in ./best_model/stage_1_model_mixed_language/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the best model manually if it's better than the previous best\n",
    "model_path = os.path.join('./best_model', 'stage_1_model_upsampling') \n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model with dataset augmented with mixed back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_df, dev_df = load_datasets()\n",
    "trdf1 = pd.read_csv('mixed_augmented_train_set.csv')\n",
    "train_set = PCLDataset(tokenizer, trdf1)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1543dff153c4fbfa92e43de670858cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RoBERTaForPCL: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RoBERTaForPCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoBERTaForPCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RoBERTaForPCL were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11372\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3560\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 28942849 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240304_144443-hf8hz2ru</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alans-team/huggingface/runs/hf8hz2ru\" target=\"_blank\">roberta-finetuning-test</a></strong> to <a href=\"https://wandb.ai/alans-team/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3560' max='3560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3560/3560 12:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff245f3d9884dc2a1aff216db67acf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-356\n",
      "Configuration saved in ./results/checkpoint-356/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9130850047755492, F1 Score: 0.4615384615384615, Precision: 0.5611510791366906, Recall: 0.39195979899497485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-356/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-356/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-356/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30408307aa340e68e67aac06d21039b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-712\n",
      "Configuration saved in ./results/checkpoint-712/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9173829990448902, F1 Score: 0.3107569721115538, Precision: 0.75, Recall: 0.19597989949748743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-712/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-712/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-712/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e37cd75ccf4fdfb89f9b6c4a3bc228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-1068\n",
      "Configuration saved in ./results/checkpoint-1068/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9173829990448902, F1 Score: 0.4175084175084175, Precision: 0.6326530612244898, Recall: 0.31155778894472363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1068/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1068/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1068/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-712] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dda57f5b2d474e8b22882e0c0553e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-1424\n",
      "Configuration saved in ./results/checkpoint-1424/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9097421203438395, F1 Score: 0.5378973105134475, Precision: 0.5238095238095238, Recall: 0.5527638190954773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1424/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1424/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1424/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-356] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefe7e22733d4b4d8e0f0580011b8faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-1780\n",
      "Configuration saved in ./results/checkpoint-1780/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9164278892072588, F1 Score: 0.56575682382134, Precision: 0.5588235294117647, Recall: 0.5728643216080402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1780/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1780/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1780/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1068] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a9e7f75c984e38bc7795d4d312be43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-2136\n",
      "Configuration saved in ./results/checkpoint-2136/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9140401146131805, F1 Score: 0.5287958115183247, Precision: 0.5519125683060109, Recall: 0.507537688442211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2136/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2136/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2136/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1424] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5792c3a6eb4cca8f2efd7dff04399e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-2492\n",
      "Configuration saved in ./results/checkpoint-2492/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9169054441260746, F1 Score: 0.5, Precision: 0.5838926174496645, Recall: 0.4371859296482412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2492/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2492/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2492/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2136] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31180ad908bf4b0494b149aaacb70ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-2848\n",
      "Configuration saved in ./results/checkpoint-2848/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9154727793696275, F1 Score: 0.522911051212938, Precision: 0.563953488372093, Recall: 0.48743718592964824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2848/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2848/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2848/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2492] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17420c67f556442589f659dfcb813a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-3204\n",
      "Configuration saved in ./results/checkpoint-3204/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9164278892072588, F1 Score: 0.5179063360881543, Precision: 0.573170731707317, Recall: 0.4723618090452261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3204/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3204/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3204/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2848] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1bfecf5bbb48478654fc018490c705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-3560\n",
      "Configuration saved in ./results/checkpoint-3560/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9169054441260746, F1 Score: 0.521978021978022, Precision: 0.5757575757575758, Recall: 0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3560/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3560/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3560/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3204] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1780 (score: 0.56575682382134).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4dab8485fcb42f4b4bb9bda9d745af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9164278892072588, F1 Score: 0.56575682382134, Precision: 0.5588235294117647, Recall: 0.5728643216080402\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "#cache_dir = os.path.join(content_path, 'cache')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    weight_decay=0,\n",
    "    lr_scheduler_type='cosine',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-finetuning-test\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "\n",
    "model = RoBERTaForPCL.from_pretrained('roberta-base', dropout_rate=0, num_frozen_layers=8).to(device)\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "# Initialize Trainer\n",
    "trainer = PCLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=dev_set,\n",
    "    data_collator=train_set.collate_fn,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./best_model/stage_1_model_mixed_language/config.json\n",
      "Model weights saved in ./best_model/stage_1_model_mixed_language/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the best model manually if it's better than the previous best\n",
    "model_path = os.path.join('./best_model', 'stage_1_model_mixed_language') \n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
