{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (6.16.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (5.8.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.4)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mEnabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.23.4)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.12.1+cu116)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from accelerate) (0.12.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (3.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2019.11.28)\n",
      "Installing collected packages: safetensors, accelerate\n",
      "Successfully installed accelerate-0.27.2 safetensors-0.4.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install protobuf==3.20.*\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    if torch.backends.mps.is_available():\n",
    "        DEVICE = 'mps'\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "else:\n",
    "    DEVICE = 'cuda:0'\n",
    "print(\"Device:\", DEVICE)\n",
    "device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (8.0.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (6.16.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (5.8.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Initialization Cell\n",
    "WORKING_ENV = 'PAPERSPACE' # Can be LABS, COLAB, PAPERSPACE, SAGEMAKER\n",
    "USERNAME = '' # If working on Lab Machines - Your college username\n",
    "assert WORKING_ENV in ['LABS', 'COLAB', 'PAPERSPACE', 'SAGEMAKER']\n",
    "\n",
    "if WORKING_ENV == 'PAPERSPACE': # Using Paperspace\n",
    "    !pip install ipywidgets\n",
    "    content_path = '/notebooks/'\n",
    "    data_path = './data/'\n",
    "    \n",
    "else:\n",
    "  raise NotImplementedError()\n",
    "\n",
    "content_path = Path(content_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running locally\n",
    "import os\n",
    "content_path = os.getcwd()\n",
    "data_path = f'{content_path}/data/'\n",
    "content_path = Path(content_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up data and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save predictions to an output file\n",
    "def labels2file(p, outf_path):\n",
    "\twith open(outf_path,'w') as outf:\n",
    "\t\tfor pi in p:\n",
    "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define the custom dataset class\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, is_multiclass=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.is_multiclass = is_multiclass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        # Use 'orig_label' for multi-class and 'label' for binary\n",
    "        if self.is_multiclass:\n",
    "            dict_item = {'text': item['text'], 'label': item['label'], 'orig_label': item['orig_label']}\n",
    "        else:\n",
    "            dict_item = {'text': item['text'], 'label': item['label']}\n",
    "        return dict_item\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts = [item['text'] for item in batch]\n",
    "        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        encodings['labels'] = labels\n",
    "        if self.is_multiclass:\n",
    "            orig_labels = torch.tensor([item['orig_label'] for item in batch], dtype=torch.long)\n",
    "            encodings['orig_labels'] = orig_labels\n",
    "        return encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "class RoBERTaForPCL(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, dropout_rate=0.1, num_frozen_layers=0, is_multiclass=False, extra_hidden_layer=False):\n",
    "        super().__init__(config)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.is_multiclass = is_multiclass\n",
    "        output_dim = 5 if is_multiclass else 1\n",
    "\n",
    "        if extra_hidden_layer:\n",
    "            self.classifier = torch.nn.Sequential(\n",
    "                torch.nn.Linear(config.hidden_size, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_rate),\n",
    "                torch.nn.Linear(256, output_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = torch.nn.Linear(config.hidden_size, output_dim)\n",
    "\n",
    "        # Freeze specified bottom layers\n",
    "        if num_frozen_layers > 0:\n",
    "            # Freeze embeddings if num_frozen_layers includes them\n",
    "            if num_frozen_layers >= 1:\n",
    "                for param in self.roberta.embeddings.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # Freeze bottom transformer layers as specified by num_frozen_layers\n",
    "            for layer in self.roberta.encoder.layer[:num_frozen_layers]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds,\n",
    "                               output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n",
    "                               return_dict=return_dict)\n",
    "        pooled_output = self.dropout(outputs[1])\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "def get_groupwise_lr_decay_optimizer(model, learning_rate=1e-5, weight_decay=0.01, lr_decay=0.95, num_groups=3):\n",
    "\n",
    "    n_layers = len(model.roberta.encoder.layer)  # Total number of layers\n",
    "    layers_per_group = max(n_layers // num_groups, 1)  # Ensure at least one layer per group\n",
    "\n",
    "    # Initialize grouped parameters list\n",
    "    grouped_parameters = []\n",
    "\n",
    "    # Embeddings parameters\n",
    "    embedding_decayed_lr = learning_rate * (lr_decay ** num_groups)\n",
    "    grouped_parameters.append({\"params\": model.roberta.embeddings.parameters(), 'lr': embedding_decayed_lr})\n",
    "\n",
    "    # Encoder layers parameters\n",
    "    for group_idx in range(num_groups):\n",
    "        # Calculate decayed learning rate for this group\n",
    "        decayed_lr = learning_rate * (lr_decay ** (num_groups - 1 - group_idx))\n",
    "        \n",
    "        # Calculate the start and end layer index for this group\n",
    "        start_layer = group_idx * layers_per_group\n",
    "        end_layer = (group_idx + 1) * layers_per_group if group_idx < num_groups - 1 else n_layers\n",
    "        \n",
    "        # Aggregate parameters from layers in this group\n",
    "        group_params = []\n",
    "        for layer_idx in range(start_layer, end_layer):\n",
    "            group_params.extend(list(model.roberta.encoder.layer[layer_idx].parameters()))\n",
    "        \n",
    "        grouped_parameters.append({\"params\": group_params, 'lr': decayed_lr})\n",
    "\n",
    "    # Pooler parameters\n",
    "    pooled_decayed_lr = learning_rate \n",
    "    grouped_parameters.append({\"params\": model.roberta.pooler.parameters(), 'lr': pooled_decayed_lr})\n",
    "\n",
    "    # Classifier parameters\n",
    "    classifier_lr = learning_rate \n",
    "    grouped_parameters.append({\"params\": model.classifier.parameters(), 'lr': classifier_lr})\n",
    "    \n",
    "    optimizer = AdamW(grouped_parameters, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer and evaluation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            if model.is_multiclass:\n",
    "                # Convert logits to probabilities and then to multi-class predictions\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                # Map multi-class labels to binary\n",
    "                mapped_preds = preds.clone()\n",
    "                mapped_preds[mapped_preds < 2] = 0  # Map 0,1 to 0\n",
    "                mapped_preds[mapped_preds >= 2] = 1  # Map 2,3,4 to 1\n",
    "                preds = mapped_preds\n",
    "\n",
    "            else:\n",
    "                # Convert logits to probabilities and then to binary predictions for binary classification\n",
    "                probs = torch.sigmoid(outputs).squeeze()\n",
    "                preds = (probs > 0.5).long()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Compute classification report based on actual task type\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Not PCL\", \"PCL\"], output_dict=True, zero_division=0)\n",
    "    model.train()\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "class PCLTrainer(Trainer):\n",
    "    def __init__(self, *args, optimizer_function=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if optimizer_function:\n",
    "            self.optimizer = optimizer_function(self.model)\n",
    "            \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    \n",
    "        if model.is_multiclass:\n",
    "            loss_labels = inputs.pop(\"orig_labels\")\n",
    "            outputs = model(**inputs)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(outputs, loss_labels)\n",
    "        else:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(outputs.view(-1), labels.float().view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "            \n",
    "\n",
    "    def evaluate(self, ignore_keys=None):\n",
    "        eval_results = evaluate(self.model, self.tokenizer, self.eval_dataset)\n",
    "        f1_score = eval_results['PCL']['f1-score']\n",
    "        precision = eval_results['PCL']['precision']\n",
    "        recall = eval_results['PCL']['recall']\n",
    "        accuracy = eval_results['accuracy']\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}, F1 Score: {f1_score}, Precision: {precision}, Recall: {recall}\")\n",
    "\n",
    "        # Log the results with wandb\n",
    "        wandb.log({\"eval_f1\": f1_score, \"precision\": precision, \"recall\": recall, \"accuracy\": accuracy})\n",
    "        return {\"eval_f1\": f1_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    dpm = DontPatronizeMe('.', '.')\n",
    "    dpm.load_task1()\n",
    "    trids = pd.read_csv('internal_train_par_ids.csv')\n",
    "    teids = pd.read_csv('internal_dev_par_ids.csv')\n",
    "\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    data=dpm.train_task1_df\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(trids)):\n",
    "        parid = trids.par_id[idx]\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        orig_label = int(data.loc[data.par_id == parid].orig_label.values[0])\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label,\n",
    "            'orig_label':orig_label\n",
    "        })\n",
    "\n",
    "    trdf1 = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "    traindf_majority = trdf1[trdf1['label'] == 0]\n",
    "    traindf_minority = trdf1[trdf1['label'] == 1]\n",
    "    traindf_minority_oversampled = resample(traindf_minority,\n",
    "                                   replace=True,\n",
    "                                   n_samples=len(traindf_majority),\n",
    "                                   random_state=42)\n",
    "    traindf_combined = pd.concat([traindf_majority, traindf_minority_oversampled])\n",
    "    traindf_combined = traindf_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    trdf1 = traindf_combined\n",
    "    \n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        #print(parid)\n",
    "        # select row from original dataset\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        orig_label = int(data.loc[data.par_id == parid].orig_label.values[0])\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label,\n",
    "            'orig_label':orig_label\n",
    "        })\n",
    "\n",
    "    tedf1 = pd.DataFrame(rows)\n",
    "\n",
    "    return trdf1, tedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_df, dev_df = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"roberta_finetuning.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "      'name': 'f1',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'num_train_epochs': {\n",
    "            'values': [5]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 1e-4]\n",
    "        },\n",
    "        'per_device_train_batch_size': {\n",
    "            'values': [32, 64]\n",
    "        },\n",
    "        'frozen_layers': {\n",
    "            'values': [0, 8]\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'values': [0, 0.1]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0, 0.01]\n",
    "        },\n",
    "        'scheduler': {\n",
    "            'values': ['linear', 'cosine']\n",
    "        },\n",
    "        'lr_decay': {\n",
    "            'values': [0.8, 0.9, 0.95]\n",
    "        },\n",
    "        'num_groups': {\n",
    "            'values': [1, 2, 4, 12]\n",
    "        },\n",
    "        'is_multiclass': {\n",
    "            'values': [False, True]\n",
    "        },\n",
    "        'extra_hidden_layer': {\n",
    "            'values': [False, True]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        torch.manual_seed(6)\n",
    "\n",
    "        # Load the datasets\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        train_set = PCLDataset(tokenizer, train_df, is_multiclass=config.is_multiclass)\n",
    "        dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=config.is_multiclass)\n",
    "        dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=config.scheduler,\n",
    "            overwrite_output_dir=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            report_to=\"wandb\",\n",
    "            run_name=\"roberta-finetuning-test\",\n",
    "            remove_unused_columns=False,\n",
    "            logging_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_f1\",\n",
    "            greater_is_better=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "\n",
    "        model = RoBERTaForPCL.from_pretrained('roberta-base', dropout_rate=config.dropout_rate, \n",
    "                                              num_frozen_layers=config.frozen_layers, is_multiclass=config.is_multiclass, \n",
    "                                              extra_hidden_layer=config.extra_hidden_layer).to(device)\n",
    "\n",
    "        def optimizer_function(model):\n",
    "            return get_groupwise_lr_decay_optimizer(\n",
    "                model, \n",
    "                learning_rate=config.learning_rate, \n",
    "                weight_decay=config.weight_decay, \n",
    "                lr_decay=config.lr_decay,\n",
    "                num_groups=config.num_groups\n",
    "    )\n",
    "\n",
    "\n",
    "        print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        # Initialize Trainer\n",
    "        trainer = PCLTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_set,\n",
    "            eval_dataset=dev_set,\n",
    "            data_collator=train_set.collate_fn,\n",
    "            tokenizer=tokenizer,\n",
    "            optimizer_function=optimizer_function\n",
    "        )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate()\n",
    "\n",
    "        # Save the best model manually if it's better than the previous best\n",
    "        if results[\"eval_f1\"] > wandb.run.summary.get('best_f1', 0):\n",
    "            wandb.run.summary['best_f1'] = results[\"eval_f1\"]\n",
    "            model_path = os.path.join('./best_model', wandb.run.name) \n",
    "            model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP_CW_Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id=\"6xm4y1gh\", function=tune_hyperparameters, count=10, project=\"NLP_CW_Final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training three best models and Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final configuration 1:\n",
    "- Number of training epochs: 20 (saving the best model along the way)\n",
    "- Use multiclass labels: False\n",
    "- Batch size: 64\n",
    "- Learning rate: 1e-4\n",
    "- Weight decay: 0.01\n",
    "- Dropout rate in linear layers: 0.1\n",
    "- LR scheduler: Cosine\n",
    "- Number of frozen layers: 8\n",
    "- Extra linear layer: True\n",
    "- Number of layer groups: 1\n",
    "- Layer-wise decay rate: 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_set = PCLDataset(tokenizer, train_df, is_multiclass=False)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=False)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-finetuning-final-first\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "\n",
    "model = RoBERTaForPCL.from_pretrained('roberta-base', dropout_rate=0.1, \n",
    "                                        num_frozen_layers=8, is_multiclass=False, \n",
    "                                        extra_hidden_layer=True).to(device)\n",
    "\n",
    "def optimizer_function(model):\n",
    "    return get_groupwise_lr_decay_optimizer(\n",
    "        model, \n",
    "        learning_rate=1e-4, \n",
    "        weight_decay=0.01, \n",
    "        lr_decay=0.95,\n",
    "        num_groups=1\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "# Initialize Trainer\n",
    "trainer = PCLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=dev_set,\n",
    "    data_collator=train_set.collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer_function=optimizer_function\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./best_model/final_model_1/config.json\n",
      "Model weights saved in ./best_model/final_model_1/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('./best_model', 'final_model_1') \n",
    "trainer.model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final configuration 2:\n",
    "- Number of training epochs: 20 (saving the best model along the way)\n",
    "- Use multiclass labels: False\n",
    "- Batch size: 64\n",
    "- Learning rate: 1e-4\n",
    "- Weight decay: 0.01\n",
    "- Dropout rate in linear layers: 0\n",
    "- LR scheduler: Cosine\n",
    "- Number of frozen layers: 0\n",
    "- Extra linear layer: False\n",
    "- Number of layer groups: 12\n",
    "- Layer-wise decay rate: 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RoBERTaForPCL: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RoBERTaForPCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoBERTaForPCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RoBERTaForPCL were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11372\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 534\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 124646401 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malan-picucci\u001b[0m (\u001b[33malans-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240304_202607-10vywbrg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alans-team/huggingface/runs/10vywbrg\" target=\"_blank\">roberta-finetuning-final</a></strong> to <a href=\"https://wandb.ai/alans-team/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 06:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e74b8fb98b84813b0140c465e29c479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-178\n",
      "Configuration saved in ./results/checkpoint-178/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8806112702960841, F1 Score: 0.5567375886524824, Precision: 0.4301369863013699, Recall: 0.7889447236180904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-178/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-178/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-178/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1602] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363005d20a80478aa12b90ec9900bc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-356\n",
      "Configuration saved in ./results/checkpoint-356/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9173829990448902, F1 Score: 0.5435356200527706, Precision: 0.5722222222222222, Recall: 0.5175879396984925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-356/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-356/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-356/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3026] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf14890ad1548ddb1bc72ca760f2b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-534\n",
      "Configuration saved in ./results/checkpoint-534/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9192932187201528, F1 Score: 0.5867970660146699, Precision: 0.5714285714285714, Recall: 0.6030150753768844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-534/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-534/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-534/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-178] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-534 (score: 0.5867970660146699).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3de0a80f9a4859897ff9cfc11c21c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9192932187201528, F1 Score: 0.5867970660146699, Precision: 0.5714285714285714, Recall: 0.6030150753768844\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_set = PCLDataset(tokenizer, train_df, is_multiclass=False)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=False)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-finetuning-final\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "\n",
    "model = RoBERTaForPCL.from_pretrained('roberta-base', dropout_rate=0, \n",
    "                                        num_frozen_layers=0, is_multiclass=False, \n",
    "                                        extra_hidden_layer=False).to(device)\n",
    "\n",
    "def optimizer_function(model):\n",
    "    return get_groupwise_lr_decay_optimizer(\n",
    "        model, \n",
    "        learning_rate=1e-4, \n",
    "        weight_decay=0.01, \n",
    "        lr_decay=0.8,\n",
    "        num_groups=12\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "# Initialize Trainer\n",
    "trainer = PCLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=dev_set,\n",
    "    data_collator=train_set.collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer_function=optimizer_function\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('./best_model', 'final_model_2') \n",
    "trainer.model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final configuration 3:\n",
    "- Number of training epochs: 20 (saving the best model along the way)\n",
    "- Use multiclass labels: True\n",
    "- Batch size: 64\n",
    "- Learning rate: 5e-5\n",
    "- Weight decay: 0\n",
    "- Dropout rate in linear layers: 0.1\n",
    "- LR scheduler: Linear\n",
    "- Number of frozen layers: 8\n",
    "- Extra linear layer: True\n",
    "- Number of layer groups: 12\n",
    "- Layer-wise decay rate: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_set = PCLDataset(tokenizer, train_df, is_multiclass=True)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=True)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    weight_decay=0,\n",
    "    lr_scheduler_type='linear',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-finetuning-final-third\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "\n",
    "model = RoBERTaForPCL.from_pretrained('roberta-base', dropout_rate=0.1, \n",
    "                                        num_frozen_layers=8, is_multiclass=True, \n",
    "                                        extra_hidden_layer=True).to(device)\n",
    "\n",
    "def optimizer_function(model):\n",
    "    return get_groupwise_lr_decay_optimizer(\n",
    "        model, \n",
    "        learning_rate=5e-5, \n",
    "        weight_decay=0, \n",
    "        lr_decay=0.9,\n",
    "        num_groups=12\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "# Initialize Trainer\n",
    "trainer = PCLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=dev_set,\n",
    "    data_collator=train_set.collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer_function=optimizer_function\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('./best_model', 'final_model_3') \n",
    "trainer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models, tokenizer, data_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = batch['label'].to(device)\n",
    "        batch_probs = []  # Store probabilities for this batch\n",
    "\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                if model.is_multiclass:\n",
    "                    # Convert logits to probabilities and then to multi-class predictions\n",
    "                    probs_multiclass = torch.softmax(outputs, dim=1)\n",
    "                    #Map classes [0,1] to 0 and classes [2,3,4] to 1\n",
    "                    probs = probs_multiclass[:, 2] + probs_multiclass[:, 3] + probs_multiclass[:, 4]  # Probability of label 1\n",
    "                    batch_probs.append(probs.cpu().numpy())  # Store binary probabilities\n",
    "\n",
    "                else:\n",
    "                    # Convert logits to probabilities and then to binary predictions for binary classification\n",
    "                    probs = torch.sigmoid(outputs).squeeze()\n",
    "                    batch_probs.append(probs.cpu().numpy())\n",
    "\n",
    "        # Average the probabilities across models for this batch\n",
    "        avg_probs = np.mean(batch_probs, axis=0)\n",
    "        # Convert averaged probabilities to binary predictions\n",
    "        aggregated_preds = (avg_probs > 0.5).astype(int)\n",
    "        \n",
    "        all_preds.extend(aggregated_preds)\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Compute classification report based on actual task type\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Not PCL\", \"PCL\"], output_dict=True, zero_division=0)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "\n",
    "model_path = './best_model/final_model_1'\n",
    "config = RobertaConfig.from_pretrained(model_path)\n",
    "model = RoBERTaForPCL.from_pretrained(model_path, config=config, dropout_rate=0.1, num_frozen_layers=8, is_multiclass=False, extra_hidden_layer=True).to(device)\n",
    "\n",
    "model_path2 = './best_model/final_model_2'\n",
    "config2 = RobertaConfig.from_pretrained(model_path2)\n",
    "model2 = RoBERTaForPCL.from_pretrained(model_path2, config=config2, dropout_rate=0, num_frozen_layers=0, is_multiclass=False, extra_hidden_layer=False).to(device)\n",
    "\n",
    "model_path3 = './best_model/final_model_3'\n",
    "config3 = RobertaConfig.from_pretrained(model_path3)\n",
    "model3 = RoBERTaForPCL.from_pretrained(model_path, config=config3, dropout_rate=0.1, num_frozen_layers=8, is_multiclass=True, extra_hidden_layer=True).to(device)\n",
    "\n",
    "models = [model, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe07e5e02029460bb78b37e9c7138f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_set = PCLDataset(tokenizer, train_df, is_multiclass=True)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=True)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "report = ensemble(models, tokenizer, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Not PCL': {'precision': 0.9564761405348715,\n",
       "  'recall': 0.9625329815303431,\n",
       "  'f1-score': 0.9594950026301946,\n",
       "  'support': 1895},\n",
       " 'PCL': {'precision': 0.6203208556149733,\n",
       "  'recall': 0.5829145728643216,\n",
       "  'f1-score': 0.6010362694300517,\n",
       "  'support': 199},\n",
       " 'accuracy': 0.9264565425023877,\n",
       " 'macro avg': {'precision': 0.7883984980749223,\n",
       "  'recall': 0.7727237771973323,\n",
       "  'f1-score': 0.7802656360301232,\n",
       "  'support': 2094},\n",
       " 'weighted avg': {'precision': 0.9245301511847953,\n",
       "  'recall': 0.9264565425023877,\n",
       "  'f1-score': 0.9254294401149948,\n",
       "  'support': 2094}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Dev and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def load_datasets_official():\n",
    "    dpm = DontPatronizeMe('.', '.')\n",
    "    dpm.load_task1()\n",
    "    trids = pd.read_csv('train_semeval_parids-labels.csv')\n",
    "    teids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    data=dpm.train_task1_df\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(trids)):\n",
    "        parid = trids.par_id[idx]\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        orig_label = int(data.loc[data.par_id == parid].orig_label.values[0])\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label,\n",
    "            'orig_label':orig_label\n",
    "        })\n",
    "\n",
    "    trdf1 = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "    traindf_majority = trdf1[trdf1['label'] == 0]\n",
    "    traindf_minority = trdf1[trdf1['label'] == 1]\n",
    "    traindf_minority_oversampled = resample(traindf_minority,\n",
    "                                   replace=True,\n",
    "                                   n_samples=len(traindf_majority),\n",
    "                                   random_state=42)\n",
    "    traindf_combined = pd.concat([traindf_majority, traindf_minority_oversampled])\n",
    "    traindf_combined = traindf_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    trdf1 = traindf_combined\n",
    "    \n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        #print(parid)\n",
    "        # select row from original dataset\n",
    "        keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "        text = data.loc[data.par_id == parid].text.values[0]\n",
    "        orig_label = int(data.loc[data.par_id == parid].orig_label.values[0])\n",
    "        label = data.loc[data.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'community':keyword,\n",
    "            'text':text,\n",
    "            'label':label,\n",
    "            'orig_label':orig_label\n",
    "        })\n",
    "\n",
    "    tedf1 = pd.DataFrame(rows)\n",
    "\n",
    "    return trdf1, tedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = load_datasets_official()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path1 = './best_model/final_model_1'\n",
    "\n",
    "config1 = RobertaConfig.from_pretrained(model_pat1)\n",
    "model1 = RoBERTaForPCL.from_pretrained(model_path1, config=config1, dropout_rate=0.1, num_frozen_layers=8, is_multiclass=False, extra_hidden_layer=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d52c2888d564cbf881fcce871fd2669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7766f382cc5403a81627bec61ec1f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd849e38c43c4e32aff136709cca7899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7725d667c3c402f936d3058dc3552e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640c5d324e354d918299589d24d202ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Not PCL': {'precision': 0.9486514522821576,\n",
       "  'recall': 0.9651715039577836,\n",
       "  'f1-score': 0.9568401778707821,\n",
       "  'support': 1895},\n",
       " 'PCL': {'precision': 0.6024096385542169,\n",
       "  'recall': 0.5025125628140703,\n",
       "  'f1-score': 0.547945205479452,\n",
       "  'support': 199},\n",
       " 'accuracy': 0.9212034383954155,\n",
       " 'macro avg': {'precision': 0.7755305454181873,\n",
       "  'recall': 0.733842033385927,\n",
       "  'f1-score': 0.7523926916751171,\n",
       "  'support': 2094},\n",
       " 'weighted avg': {'precision': 0.915746905514316,\n",
       "  'recall': 0.9212034383954155,\n",
       "  'f1-score': 0.9179814866072317,\n",
       "  'support': 2094}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_set = PCLDataset(tokenizer, train_df, is_multiclass=False)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=False)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "report = evaluate(model1, tokenizer, dev_set)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path2 = './best_model/final_model_2'\n",
    "\n",
    "config2 = RobertaConfig.from_pretrained(model_path2)\n",
    "model2 = RoBERTaForPCL.from_pretrained(model_path2, config=config2, dropout_rate=0, num_frozen_layers=0, is_multiclass=False, extra_hidden_layer=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5544153a94f64652821ffd94c571d148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Not PCL': {'precision': 0.9483737738771296,\n",
       "  'recall': 0.9693931398416886,\n",
       "  'f1-score': 0.958768267223382,\n",
       "  'support': 1895},\n",
       " 'PCL': {'precision': 0.6305732484076433,\n",
       "  'recall': 0.49748743718592964,\n",
       "  'f1-score': 0.5561797752808989,\n",
       "  'support': 199},\n",
       " 'accuracy': 0.9245463228271251,\n",
       " 'macro avg': {'precision': 0.7894735111423865,\n",
       "  'recall': 0.7334402885138092,\n",
       "  'f1-score': 0.7574740212521405,\n",
       "  'support': 2094},\n",
       " 'weighted avg': {'precision': 0.9181721002532387,\n",
       "  'recall': 0.9245463228271251,\n",
       "  'f1-score': 0.9205089024208251,\n",
       "  'support': 2094}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_set = PCLDataset(tokenizer, train_df, is_multiclass=False)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=False)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "report = evaluate(model2, tokenizer, dev_set)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path3 = './best_model/final_model_3'\n",
    "config3 = RobertaConfig.from_pretrained(model_path3)\n",
    "model3 = RoBERTaForPCL.from_pretrained(model_path3, config=config3, dropout_rate=0.1, num_frozen_layers=8, is_multiclass=True, extra_hidden_layer=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd494774976d407399ebf129768d825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "# Load the datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_set = PCLDataset(tokenizer, train_df, is_multiclass=True)\n",
    "dev_set_PCL = PCLDataset(tokenizer, dev_df, is_multiclass=True)\n",
    "dev_set = DataLoader(dev_set_PCL, batch_size=32)\n",
    "\n",
    "report = evaluate(model3, tokenizer, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Not PCL': {'precision': 0.9627192982456141,\n",
       "  'recall': 0.9266490765171504,\n",
       "  'f1-score': 0.9443398763108363,\n",
       "  'support': 1895},\n",
       " 'PCL': {'precision': 0.48518518518518516,\n",
       "  'recall': 0.6582914572864321,\n",
       "  'f1-score': 0.5586353944562898,\n",
       "  'support': 199},\n",
       " 'accuracy': 0.9011461318051576,\n",
       " 'macro avg': {'precision': 0.7239522417153996,\n",
       "  'recall': 0.7924702669017913,\n",
       "  'f1-score': 0.7514876353835631,\n",
       "  'support': 2094},\n",
       " 'weighted avg': {'precision': 0.9173375940913517,\n",
       "  'recall': 0.9011461318051576,\n",
       "  'f1-score': 0.9076850568795781,\n",
       "  'support': 2094}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc504703a1814e78a7814542ada61f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Not PCL': {'precision': 0.9514613778705637,\n",
       "  'recall': 0.9620052770448548,\n",
       "  'f1-score': 0.9567042770926267,\n",
       "  'support': 1895},\n",
       " 'PCL': {'precision': 0.5955056179775281,\n",
       "  'recall': 0.5326633165829145,\n",
       "  'f1-score': 0.5623342175066314,\n",
       "  'support': 199},\n",
       " 'accuracy': 0.9212034383954155,\n",
       " 'macro avg': {'precision': 0.7734834979240459,\n",
       "  'recall': 0.7473342968138847,\n",
       "  'f1-score': 0.759519247299629,\n",
       "  'support': 2094},\n",
       " 'weighted avg': {'precision': 0.9176336814910442,\n",
       "  'recall': 0.9212034383954155,\n",
       "  'f1-score': 0.9192259380966319,\n",
       "  'support': 2094}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "\n",
    "model_path = './best_model/final_model_1'\n",
    "config = RobertaConfig.from_pretrained(model_path)\n",
    "model = RoBERTaForPCL.from_pretrained(model_path, config=config, dropout_rate=0.1, num_frozen_layers=8, is_multiclass=False, extra_hidden_layer=True).to(device)\n",
    "\n",
    "model_path2 = './best_model/final_model_2'\n",
    "config2 = RobertaConfig.from_pretrained(model_path2)\n",
    "model2 = RoBERTaForPCL.from_pretrained(model_path2, config=config2, dropout_rate=0, num_frozen_layers=0, is_multiclass=False, extra_hidden_layer=False).to(device)\n",
    "\n",
    "model_path3 = './best_model/final_model_3'\n",
    "config3 = RobertaConfig.from_pretrained(model_path3)\n",
    "model3 = RoBERTaForPCL.from_pretrained(model_path, config=config3, dropout_rate=0.1, num_frozen_layers=8, is_multiclass=True, extra_hidden_layer=True).to(device)\n",
    "\n",
    "models = [model, model2, model3]\n",
    "results = ensemble(models, tokenizer, dev_set)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            if model.is_multiclass:\n",
    "                # Convert logits to probabilities and then to multi-class predictions\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                # Map multi-class labels to binary\n",
    "                mapped_preds = preds.clone()\n",
    "                mapped_preds[mapped_preds < 2] = 0  # Map 0,1 to 0\n",
    "                mapped_preds[mapped_preds >= 2] = 1  # Map 2,3,4 to 1\n",
    "                preds = mapped_preds\n",
    "\n",
    "            else:\n",
    "                # Convert logits to probabilities and then to binary predictions for binary classification\n",
    "                probs = torch.sigmoid(outputs).squeeze()\n",
    "                preds = (probs > 0.5).long()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "    return all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(models, tokenizer, data_loader):\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        batch_probs = []  # Store probabilities for this batch\n",
    "\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                if model.is_multiclass:\n",
    "                    # Convert logits to probabilities and then to multi-class predictions\n",
    "                    probs_multiclass = torch.softmax(outputs, dim=1)\n",
    "                    #Map classes [0,1] to 0 and classes [2,3,4] to 1\n",
    "                    probs = probs_multiclass[:, 2] + probs_multiclass[:, 3] + probs_multiclass[:, 4]  # Probability of label 1\n",
    "                    batch_probs.append(probs.cpu().numpy())  # Store binary probabilities\n",
    "\n",
    "                else:\n",
    "                    # Convert logits to probabilities and then to binary predictions for binary classification\n",
    "                    probs = torch.sigmoid(outputs).squeeze()\n",
    "                    batch_probs.append(probs.cpu().numpy())\n",
    "\n",
    "        # Average the probabilities across models for this batch\n",
    "        avg_probs = np.mean(batch_probs, axis=0)\n",
    "        # Convert averaged probabilities to binary predictions\n",
    "        aggregated_preds = (avg_probs > 0.5).astype(int)\n",
    "        \n",
    "        all_preds.extend(aggregated_preds.tolist())\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model, model2, model3]\n",
    "dev_predictions = predict_ensemble(models, tokenizer, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2file([[k] for k in dev_predictions], 'dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2file(dev_df.label.apply(lambda x:[x]).tolist(), os.path.join('ref/', 'task1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 evaluation.py . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('task4_test.tsv', delimiter='\\t', header=None, names=['par_id', 'art_id', 'community', 'country', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a empty label column to the test data\n",
    "test_df['label'] = np.nan\n",
    "test_df['orig_label'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c1329da7a54925b04da229debc0534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set = PCLDataset(tokenizer, test_df, is_multiclass=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32)\n",
    "\n",
    "test_predictions = predict_ensemble(models, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2file([[k] for k in test_predictions], 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
